Functions
=========
* Implement function composition
  - addition
  - multiplication (scalar)
  - multiplication (matrix, vector)
  - linear in a convex function
  - convex in a convex function

Optimizers
==========
- implement steepest descent method
- Implement (L-)BFGS
- Implement primal-dual interior point solver
- implement primal-dual subgradient descent
- implement projection onto linear constraints
- implement mirror descent
* double check that conjugate gradient is working. Shouldn't it converge
  exactly for quadratic problems when the number of iterations = dimension of
  problem?

Testing
=======
- implement an example that uses general constraints

Constraints
===========
- Add better support inequality constraints. There's gotta be something better
  than a log barrier function.
- add support for finding an initial solution

Stopping Criteria
=================
- Implement some method to check convergence using duality gap

Learning Rates
==============
- implement Polyak step size

General
=======
- added symbolic problem definitions
- implement the following interface

  minimize(<expr>)
    .subject_to(<expr>)
    .subject_to(<expr>)
    .using(<algorithm>)

  where <algorithm> is initialized with all algorithm-specific parameters
- implement Lagrangian from problem definition
- deal with multiplication, addition of functions
