Functions
=========
* Implement function composition

Optimizers
==========
- implement steepest descent method
- Implement (L-)BFGS
- Implement primal-dual interior point solver
- implement primal-dual subgradient descent
- implement projection onto linear constraints
* double check that conjugate gradient is working. Shouldn't it converge
  exactly for quadratic problems when the number of iterations = dimension of
  problem?

Testing
=======

Constraints
===========
- Add better support inequality constraints. There's gotta be something better
  than a log barrier function.
- add support for finding an initial solution

Stopping Criteria
=================
- Implement stopping criterion for Newton's method
- Implement some method to check convergence using duality gap
- Implement some convergence criteria based on size of gradient

Learning Rates
==============
- implement Polyak step size

General
=======
* fix name collisions b/w stopping criteria, learning rates, optimizers

  class LineSearch(...):
    def __init__(self, ...):
      self.learning_rate = _LineSearch(...)

* Add some ways to visually inspect convergence. Logging and plotting a must.
- added symbolic problem definitions
- implement the following interface

  minimize(<expr>)
    .subject_to(<expr>)
    .subject_to(<expr>)
    .using(<algorithm>)

  where <algorithm> is initialized with all algorithm-specific parameters
- implement Lagrangian from problem definition
- deal with multiplication, addition of functions
